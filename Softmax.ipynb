{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi2OvqLVHTvPqfirs4ndZm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simrathanspal/deep_models_from_scratch/blob/main/Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wzBZxIvI-FnS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import heapq\n",
        "\n",
        "class Softmax:\n",
        "  def naive(self, arr):\n",
        "    d = sum(math.exp(x) for x in arr)\n",
        "    return [math.exp(x)/d for x in arr]\n",
        "\n",
        "  def stable(self, arr):\n",
        "    \"\"\"\n",
        "    Extremely small or large values can make exponent value 0 or extremely large\n",
        "    This leads to instable training.\n",
        "\n",
        "    Very large value -> Numerical overflow -> treated as inf\n",
        "    inf/inf -> NaN\n",
        "\n",
        "    Very small value -> Numerical underflow -> treated as 0\n",
        "    0/0 -> inf\n",
        "\n",
        "    Solution: subtract all values by max value so that it doesn't grow very big\n",
        "    \"\"\"\n",
        "\n",
        "    m = max(arr)\n",
        "    d = sum(math.exp(x-m) for x in arr)\n",
        "    return [math.exp(x-m)/d for x in arr]\n",
        "\n",
        "  def online(self, arr):\n",
        "    \"\"\"\n",
        "    In Transformers we compute Softmax at multiple places\n",
        "    1) Attention\n",
        "    2) Next token prediction from FFN\n",
        "\n",
        "    The softmax in attention layer causes the max inefficiency because\n",
        "    the attention matrix is NxN where N is the length of the sequence.\n",
        "\n",
        "    Every token attends to every other token.\n",
        "\n",
        "    So, for computing Softmax for every token we go over the whole sequence.\n",
        "    And, we do this for the whole sequence.\n",
        "    For n_heads * n_layers. Which makes it a bottle neck.\n",
        "\n",
        "    Hence the Softmax optimization was introduced in Flash Attention.\n",
        "    The key idea is to take small chunks of the data and keep a running sum and\n",
        "    max so far and adjust the previous chunk values.\n",
        "    \"\"\"\n",
        "    m = float(\"-inf\")\n",
        "    d = 0.0\n",
        "    tile_size = 2\n",
        "\n",
        "    for i in range(0, len(arr), tile_size):\n",
        "      tile = arr[i:i+tile_size]\n",
        "\n",
        "      # Compute running sum for the tile\n",
        "      m_tile = max(tile)\n",
        "      d_tile = sum(math.exp(x-m_tile) for x in tile)\n",
        "\n",
        "      # Update variables\n",
        "      m_prev = m\n",
        "      m = max(m, m_tile)\n",
        "\n",
        "      d = d*math.exp(m_prev - m) + d_tile*math.exp(m_tile - m)\n",
        "\n",
        "    return [math.exp(x-m)/d for x in arr]\n",
        "\n",
        "  def online_topk(self, arr, k):\n",
        "    \"\"\"\n",
        "    Introduced by the paper \"Online Normalizer function for Softmax\" by Nvidia.\n",
        "    Softmax for output prediction becomes expensive when we are running it\n",
        "    repeated for Beam Search.\n",
        "\n",
        "    Inefficiencies\n",
        "    1) Beam search will sample multiple times\n",
        "    2) For every prediction we need top k but we find softmax over the whole vocabulary.\n",
        "\n",
        "    Solution: Min Heap + Online Softmax\n",
        "    \"\"\"\n",
        "    m = float(\"-inf\")\n",
        "    d = 0.0\n",
        "    tile_size = 2\n",
        "    min_heap = []\n",
        "\n",
        "    for i in range(0, len(arr), tile_size):\n",
        "      tile = arr[i:i+tile_size]\n",
        "\n",
        "      # Calculate for the tile\n",
        "      m_tile = max(tile)\n",
        "      d_tile = sum(math.exp(x-m_tile) for x in tile)\n",
        "\n",
        "      # Update values\n",
        "      m_prev = m\n",
        "      m = max(m, m_tile)\n",
        "\n",
        "      # Adjust values\n",
        "      d = d* math.exp(m_prev - m) + d_tile* math.exp(m_tile -m)\n",
        "\n",
        "      for j,x in enumerate(tile):\n",
        "        if len(min_heap)<k:\n",
        "          heapq.heappush(min_heap, (x, i+j))\n",
        "        elif x > min_heap[0][0]:\n",
        "          heapq.heapreplace(min_heap, (x, i+j))\n",
        "\n",
        "    min_heap.sort(key = lambda x: x[0], reverse = True)\n",
        "    top_prob = [math.exp(x-m)/d for x,j in min_heap]\n",
        "    top_ids = list(map(lambda x: x[1], min_heap))\n",
        "\n",
        "    return top_prob, top_ids\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dw1ifyuvLnKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}